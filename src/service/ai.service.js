const { createOpenAI } = require('@ai-sdk/openai');
const { streamText, convertToModelMessages } = require('ai');
const { ollamaBaseURL } = require('../constants/urls');
const Utils = require('../utils/index'); // å¼•å…¥ Utils å®ä¾‹
// åˆ›å»º Ollama çš„ OpenAI å…¼å®¹å®ä¾‹
// å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®è¿œç¨‹ Ollama æœåŠ¡å™¨
// æœ¬åœ°: http://localhost:11434/v1
// è¿œç¨‹(winæœ¬): http://192.168.3.10:11434/v1
const ollama = createOpenAI({
  baseURL: ollamaBaseURL,
  apiKey: 'ollama' // Ollama ä¸éœ€è¦çœŸå®çš„ API keyï¼Œä½† SDK è¦æ±‚æä¾›
});

console.log(`Ollama æœåŠ¡åœ°å€: ${ollamaBaseURL}`);

class AiService {
  // å¥åº·æ£€æŸ¥ï¼šæµ‹è¯• Ollama æœåŠ¡æ˜¯å¦å¯ç”¨
  checkHealth = async () => {
    try {
      // å°è¯•è·å–æ¨¡å‹åˆ—è¡¨æ¥éªŒè¯æœåŠ¡æ˜¯å¦å¯ç”¨
      const baseUrl = ollamaBaseURL.replace('/v1', '');
      const res = await fetch(`${baseUrl}/api/tags`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000) // è®¾å®š5ç§’è¶…æ—¶
      });

      if (!res.ok) {
        throw new Error(`Health check failed with status: ${res.status}`);
      }

      const data = await res.json();
      // æŒ‰éƒ¨ç½²æ—¶é—´å€’åºæ’åºï¼Œå…ˆéƒ¨ç½²çš„æ¨¡å‹æ’åœ¨å‰é¢
      const models = data.models
        .toSorted((a, b) => new Date(a.modified_at) - new Date(b.modified_at))
        .map(({ model }) => ({
          name: model.split(':')[0],
          value: model
        }));
      console.log(`âœ… [Health Check] Ollama is running, models: ${JSON.stringify(models)}`);
      return [true, models];
    } catch (error) {
      console.warn(`âš ï¸ [Health Check] Ollama is not available:`, error.message);
      return [false, []];
    }
  };

  /**
   * æµå¼å¯¹è¯æ¥å£
   * @param {Array} messages - æ¶ˆæ¯å†å² [{role: 'user', content: '...'}, ...]
   * @param {String} model - æ¨¡å‹åç§°, é»˜è®¤ä¸º qwen2.5:7b
   * @param {String} context - æ–‡ç« å†…å®¹ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
   */
  streamChat = async (messages, model = 'qwen2.5:7b', context = null) => {
    // streamChat = async (messages, model = 'deepseek-r1:8b', context = null) => {
    try {
      console.log(`\nğŸ¤– [AI Request] æ¨¡å‹: ${model}, æ¶ˆæ¯æ•°: ${messages.length}`);
      const startTime = Date.now();

      // æ„å»ºç³»ç»Ÿæç¤º
      let systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿è§£é‡Šä»£ç ã€æ€»ç»“æ–‡ç« å’Œå›ç­”æŠ€æœ¯é—®é¢˜ã€‚';
      // å¦‚æœæœ‰æ–‡ç« ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºä¸­
      if (context) {
        // 1. æ¸…ç† HTML æ ‡ç­¾ (ä½¿ç”¨ Utils ä¸­ä¿ç•™ç»“æ„çš„æ¸…æ´—æ–¹æ³•)
        let cleanContext = Utils.cleanTextForAI(context);

        // console.log('ğŸ§¹ [AI Service] HTML å†…å®¹å·²æ¸…ç†, åŸå§‹é•¿åº¦:', context.length, '-> æ¸…ç†å:', cleanContext.length);

        // 2. é™åˆ¶æ–‡ç« å†…å®¹é•¿åº¦ï¼Œé¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£
        // Qwen2.5 æ”¯æŒè¾ƒé•¿ä¸Šä¸‹æ–‡ (é€šå¸¸ 32k-128k)ï¼Œè¿™é‡Œè®¾ç½®æ›´å®½æ¾çš„é™åˆ¶
        const maxContextLength = 50000; // çº¦ 50000 å­—ç¬¦ (å®‰å…¨èŒƒå›´å†…)
        const truncatedContext = cleanContext.length > maxContextLength ? cleanContext.substring(0, maxContextLength) + '...\n[æ–‡ç« å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]' : cleanContext;

        systemPrompt += `\n\nå½“å‰æ–‡ç« å†…å®¹ï¼š\n${truncatedContext}\n\nè¯·åŸºäºè¿™ç¯‡æ–‡ç« çš„å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚`;
      }

      // å¯¹è¯å†å²ç®¡ç†ï¼šä¿ç•™æœ€è¿‘çš„ 10 è½®å¯¹è¯ï¼ˆ20 æ¡æ¶ˆæ¯ï¼‰
      const maxMessages = 20;
      const managedMessages = messages.length > maxMessages ? messages.slice(-maxMessages) : messages;

      const result = await streamText({
        model: ollama.chat(model), // æ˜ç¡®ä½¿ç”¨ chat æ–¹æ³•
        system: systemPrompt,
        messages: convertToModelMessages(managedMessages),
        // å¯é€‰ï¼šè®¾ç½®æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆéœ€è¦ Ollama æ”¯æŒï¼‰
        maxTokens: 4096 // æœ€å¤§è¾“å‡º tokens
      });

      const endTime = Date.now();
      console.log(`âœ… [AI Response] è¯·æ±‚å®Œæˆ, è€—æ—¶: ${endTime - startTime}ms`);

      // ç›´æ¥è¿”å› result å¯¹è±¡ï¼Œè®© Controller å¤„ç†å“åº”æµ
      return result;
    } catch (error) {
      console.error('âŒ [AI Service Error]', error);

      // è¯¦ç»†çš„é”™è¯¯åˆ†ç±»å’Œå‹å¥½æç¤º
      let errorMessage = 'AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨';
      let errorCode = 'AI_SERVICE_ERROR';

      // æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„æç¤º
      // æ³¨ï¼šä»¥ä¸‹é”™è¯¯ç æ˜¯ Node.js ç³»ç»Ÿçº§ç½‘ç»œé”™è¯¯ç ï¼ˆéæ¡†æ¶ç‰¹æœ‰ï¼‰
      // ECONNREFUSED - è¿æ¥è¢«æ‹’ç»ï¼ˆç›®æ ‡ç«¯å£æœªç›‘å¬ï¼‰
      // ETIMEDOUT - è¿æ¥è¶…æ—¶ï¼ˆç½‘ç»œå»¶è¿Ÿæˆ–æœåŠ¡å™¨æ— å“åº”ï¼‰
      // ENOTFOUND - DNS è§£æå¤±è´¥ï¼ˆä¸»æœºåä¸å­˜åœ¨ï¼‰
      if (error.message.includes('ECONNREFUSED') || error.message.includes('connect')) {
        errorMessage = `AI æœåŠ¡å™¨è¿æ¥å¤±è´¥ (${ollamaBaseURL})ã€‚è¯·æ£€æŸ¥ï¼š1. Ollama æ˜¯å¦æ­£åœ¨è¿è¡Œï¼Ÿ2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ï¼Ÿ3. æœåŠ¡å™¨åœ°å€æ˜¯å¦æ­£ç¡®ï¼Ÿ`;
        errorCode = 'CONNECTION_REFUSED';
      } else if (error.message.includes('timeout') || error.message.includes('ETIMEDOUT')) {
        errorMessage = 'AI æœåŠ¡å™¨å“åº”è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•';
        errorCode = 'TIMEOUT';
      } else if (error.message.includes('ENOTFOUND')) {
        errorMessage = `æ— æ³•è§£æ AI æœåŠ¡å™¨åœ°å€ (${ollamaBaseURL})ï¼Œè¯·æ£€æŸ¥é…ç½®`;
        errorCode = 'HOST_NOT_FOUND';
      } else if (error.message.includes('model')) {
        errorMessage = `æ¨¡å‹ "${model}" æœªæ‰¾åˆ°ï¼Œè¯·å…ˆä¸‹è½½: ollama pull ${model}`;
        errorCode = 'MODEL_NOT_FOUND';
      }

      const customError = new Error(errorMessage);
      customError.code = errorCode;
      customError.originalError = error.message;
      throw customError;
    }
  };
}

module.exports = new AiService();
